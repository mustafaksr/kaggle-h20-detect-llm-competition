{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style=\"border: 2px solid #555; color:black; border-radius: 10px; background-color: #0074D9; padding: 10px; font-size: 20px; text-align: center;\">Introduction</div>","metadata":{}},{"cell_type":"markdown","source":"**Table Of Content:**\n* [Introduction](#1)\n* [Refactor and Define utils](#2)\n* [Refactor Train](#3)\n* [Sweeps](#4)\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# <div style=\"border: 2px solid #555; color:black; border-radius: 10px; background-color: #0074D9; padding: 10px; font-size: 20px; text-align: center;\">Refactor and Define utils</div>\n* [return top](#1)","metadata":{}},{"cell_type":"code","source":"%%writefile utils.py\nimport json\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport torch\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer, EarlyStoppingCallback\nimport torch.nn.functional as F\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport torch\nfrom datasets import Dataset\nimport io\n\ndef read_data():\n    \"\"\"\n    return train , test\n    \"\"\"\n    with open(\"/kaggle/working/artifacts/detect_llm_raw_data:v1/train_df.table.json\") as json_data:\n        data = json.load(json_data)\n        train = pd.DataFrame(data = data[\"data\"],columns=data[\"columns\"])\n        json_data.close()\n\n    with open(\"/kaggle/working/artifacts/detect_llm_raw_data:v1/test_df.table.json\") as json_data:\n        data = json.load(json_data)\n        test = pd.DataFrame(data = data[\"data\"],columns=data[\"columns\"])\n        json_data.close()\n    return train , test\n\ndef preprocess(train=None,test=None):\n    \"\"\"\n    return dataset_train, dataset_test\n    \"\"\"\n    train.fillna(\" \",inplace=True)\n    test.fillna(\" \",inplace=True)\n    train[\"text\"] = train[\"Question\"] + \" \" + train[\"Response\"]\n    test[\"text\"] = test[\"Question\"] + \" \" + test[\"Response\"]\n    df_train = train[[\"target\",\"text\"]]\n    df_test = test[[\"text\"]]\n    dataset_train = Dataset.from_pandas(df_train)\n    dataset_test = Dataset.from_pandas(df_test)\n    \n    return dataset_train, dataset_test\n\n\ndef dataset_tokenize_n_split(train, dataset_train, dataset_test,model_name):\n    \"\"\"\n    return split_train_dataset,split_eval_dataset , tokenized_test , tokenizer\n    \"\"\"\n    tokenizer       = AutoTokenizer.from_pretrained(model_name )\n    def tokenize_function(examples):\n    \n        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n    tokenized_train = dataset_train.map(tokenize_function, batched=True)\n    tokenized_test  = dataset_test.map(tokenize_function, batched=True)\n    tokenized_train = tokenized_train.remove_columns(['text'])\n    tokenized_train = tokenized_train.rename_column(\"target\", \"labels\")\n    tokenized_test = tokenized_test.remove_columns(['text'])\n\n    kf= StratifiedKFold(n_splits=10,shuffle=True,random_state=42)\n    for i , (tr_idx,val_idx) in enumerate(kf.split(train,train.target)):\n        print(f\"Fold : {i}\")\n        print(f\"shape train : {tr_idx.shape}\")\n        print(f\"shape val : {val_idx.shape}\")\n        break\n        \n    \n    split_train_dataset = tokenized_train.select(tr_idx)\n    split_eval_dataset = tokenized_train.select(val_idx)\n\n    return split_train_dataset,split_eval_dataset , tokenized_test , tokenizer\n\ndef predict_fn(dataset_ = None):\n    \n    \"\"\"\n    return mean of all_probabilities (m,7)\n    \"\"\"\n    input_ids = dataset_['input_ids']\n    # token_type_ids = dataset_['token_type_ids']\n    attention_mask = dataset_['attention_mask']\n\n    # Move the input tensors to the GPU\n    input_ids = torch.tensor(input_ids).to('cuda:0')\n    # token_type_ids = torch.tensor(token_type_ids).to('cuda:0')\n    attention_mask = torch.tensor(attention_mask).to('cuda:0')\n\n    # Define batch size\n    batch_size = 8\n\n    # Calculate the number of batches\n    num_samples = len(input_ids)\n    num_batches = (num_samples + batch_size - 1) // batch_size\n\n    # Initialize a list to store the softmax probabilities\n    all_probabilities = []\n\n    # Make predictions in batches\n    with torch.no_grad():\n        for batch in range(num_batches):\n            start_idx = batch * batch_size\n            end_idx = min((batch + 1) * batch_size, num_samples)\n\n            batch_input_ids = input_ids[start_idx:end_idx]\n    #         batch_token_type_ids = token_type_ids[start_idx:end_idx]\n            batch_attention_mask = attention_mask[start_idx:end_idx]\n\n            outputs = model(input_ids=batch_input_ids, \n    #                         token_type_ids=batch_token_type_ids, \n                            attention_mask=batch_attention_mask)\n            logits = outputs.logits\n\n            # Apply softmax to get probabilities\n            probabilities = F.softmax(logits, dim=1)\n\n\n            all_probabilities.extend(probabilities.tolist())\n    return np.concatenate(all_probabilities,axis=0).reshape(dataset_.shape[0],7)\n\n\ndef conf_mat(df_val = None,preds_val = None):\n    \"\"\"\n    no return\n    \"\"\"\n    plt.figure(figsize=(8,8))\n    ConfusionMatrixDisplay.from_predictions(df_val.target,np.argmax(preds_val,axis=1))\n    plt.savefig(f\"val_conf_matrix.png\", format=\"png\")\n    plt.show();\n    conf = wandb.Image(data_or_path=\"val_conf_matrix.png\")\n    wandb.log({\"val_conf_matrix\": conf})\ndef create_model(model_name = \"distilroberta-base\",num_labels = 7):\n    \"\"\"\n    return\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n    # Specify the GPU device\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    # Move your model to the GPU\n    model.to(device);\n    \n    return model\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-11-09T09:58:07.727382Z","iopub.execute_input":"2023-11-09T09:58:07.727735Z","iopub.status.idle":"2023-11-09T09:58:07.743003Z","shell.execute_reply.started":"2023-11-09T09:58:07.727704Z","shell.execute_reply":"2023-11-09T09:58:07.742046Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Writing utils.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# <div style=\"border: 2px solid #555; color:black; border-radius: 10px; background-color: #0074D9; padding: 10px; font-size: 20px; text-align: center;\">Refactor Train</div>\n* [return top](#1)","metadata":{}},{"cell_type":"code","source":"# %%writefile train.py\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport torch\nfrom datasets import Dataset\nimport json\nfrom IPython.display import display\nimport wandb\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nfrom transformers import AutoModelForSequenceClassification,TrainerCallback\nfrom transformers import TrainingArguments, Trainer, EarlyStoppingCallback\nimport torch.nn.functional as F\nfrom utils import *\nimport io\n\nclass WandbMetricsLogger(TrainerCallback):\n    def on_evaluate(self, args, state, control, model, metrics):\n        # Log metrics to Wandb\n        wandb.log(metrics)\n        \ndefault_config = {\n        'method': 'random',\n        'metric': {\n        'goal': 'minimize', \n        'name': 'eval_loss'\n        },\n    }\n\n\n    # hyperparameters\nparameters_dict = {\n        'epochs': {\n            'value': 2\n            },\n        'seed': {\n            'value': 42\n            },\n        'batch_size': {\n            'values': [4, 8, 16]\n            },\n        'learning_rate': {\n            'distribution': 'log_uniform_values',\n            'min': 1e-4,\n            'max': 2e-3\n        },\n        'weight_decay': {\n            'values': [0.0, 0.2]\n        },\n        'learning_sch': {\n            'values': ['linear','polynomial','cosine']\n        },\n        'architecture': {\n            'values': [\"distilroberta-base\",\"bert-base-uncased\",\"distilbert-base-uncased\"]\n        },\n    }\n\n\ndefault_config['parameters'] = parameters_dict\n\ndef compute_metrics_fn(eval_preds):\n    metrics = dict()\n\n    # Extract the validation loss from eval_preds\n    validation_loss = eval_preds.loss\n    metrics['validation_loss'] = validation_loss\n\n    return metrics\n\ndef parse_args():\n    \"Overriding default argments\"\n    argparser = argparse.ArgumentParser(description='Process hyper-parameters')\n    argparser.add_argument('--batch_size', type=int, default=default_config.get(\"parameters\").get(\"batch_size\").get(\"values\")[-1],\n                           help='batch size')\n    argparser.add_argument('--epochs', type=int, default=default_config.get(\"parameters\").get(\"epochs\").get(\"value\"),\n                           help='number of training epochs')\n    argparser.add_argument('--lr', type=float, default=default_config.get(\"parameters\").get(\"learning_rate\").get(\"min\"),\n                           help='learning rate')\n    argparser.add_argument('--seed', type=int, default=default_config.get(\"parameters\").get(\"seed\").get(\"value\"),\n                           help='random seed')\n    argparser.add_argument('--weight_decay', type=float, default=default_config.get(\"parameters\").get(\"weight_decay\").get(\"values\")[-1],\n                           help='random seed')\n    \n    args = argparser.parse_args()\n    vars(default_config).update(vars(args))\n    return\n\n\n\ndef train(config=None):\n    \n    torch.manual_seed(default_config.get(\"parameters\").get(\"seed\").get(\"value\"))\n    \n    run = wandb.init(\n                project=\"h2o-ai-predict-the-llm-kaggle-competition\", \n                entity=None, \n                   job_type=\"hyperparameter-tuning\"\n    )\n    if \"artifacts\" not in os.listdir():\n        raw_data_at = run.use_artifact('mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/detect_llm_raw_data:v1', \n                                                       type='raw_data')\n        artifact_di = raw_data_at.download()\n    else: pass\n    train , test = read_data()\n    dataset_train, dataset_test = preprocess(train=train,test=test)\n    config = wandb.config\n    split_train_dataset,split_eval_dataset , tokenized_test , tokenizer = dataset_tokenize_n_split(train,dataset_train, dataset_test,config.architecture)\n\n    \n    \n    \n    model = create_model(model_name =config.architecture ,num_labels = 7)\n    \n    num_train_epochs=2.\n    training_args = TrainingArguments(                                \n\n                                output_dir='h2o-ai-sweeps',\n                                report_to='wandb',  # Turn on Weights & Biases logging\n                                num_train_epochs=config.epochs,\n                                learning_rate=config.learning_rate,\n                                lr_scheduler_type = config.learning_sch,\n                                per_device_train_batch_size=config.batch_size,\n                                per_device_eval_batch_size=16,\n                                save_strategy='epoch',\n                                evaluation_strategy='epoch',\n                                logging_strategy='epoch',\n                                metric_for_best_model=\"eval_loss\", \n                                load_best_model_at_end=True,\n                                remove_unused_columns=False,\n                                greater_is_better=False,\n                                weight_decay = config.weight_decay\n                                \n\n                                 )\n    early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n    trainer = Trainer(\n                        model=model,\n                        args=training_args,\n                        train_dataset=split_train_dataset,\n                        eval_dataset=split_eval_dataset,\n                        callbacks = [early_stopping],\n                        tokenizer=tokenizer,\n        )\n    trainer.train()\n\n    \n# if __name__==\"__main__\":\n# #     wandb.agent(sweep_id, train, count=20)\n#     parse_args()\n#     train(default_config)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T09:58:08.885662Z","iopub.execute_input":"2023-11-09T09:58:08.886020Z","iopub.status.idle":"2023-11-09T09:58:22.527509Z","shell.execute_reply.started":"2023-11-09T09:58:08.885989Z","shell.execute_reply":"2023-11-09T09:58:22.526681Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# <div style=\"border: 2px solid #555; color:black; border-radius: 10px; background-color: #0074D9; padding: 10px; font-size: 20px; text-align: center;\">Sweeps</div>\n* [return top](#1)","metadata":{}},{"cell_type":"code","source":"wandb.login(relogin=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T09:58:22.529389Z","iopub.execute_input":"2023-11-09T09:58:22.530527Z","iopub.status.idle":"2023-11-09T09:58:27.862078Z","shell.execute_reply.started":"2023-11-09T09:58:22.530491Z","shell.execute_reply":"2023-11-09T09:58:27.861155Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"sweep_id = wandb.sweep(default_config, project='h2o-ai-predict-the-llm-kaggle-competition')","metadata":{"execution":{"iopub.status.busy":"2023-11-09T09:58:29.620422Z","iopub.execute_input":"2023-11-09T09:58:29.621940Z","iopub.status.idle":"2023-11-09T09:58:31.712168Z","shell.execute_reply.started":"2023-11-09T09:58:29.621897Z","shell.execute_reply":"2023-11-09T09:58:31.711291Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Create sweep with ID: q9q9767w\nSweep URL: https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.agent(sweep_id, train, count=20)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-09T09:58:32.776925Z","iopub.execute_input":"2023-11-09T09:58:32.777536Z","iopub.status.idle":"2023-11-09T12:01:04.977491Z","shell.execute_reply.started":"2023-11-09T09:58:32.777508Z","shell.execute_reply":"2023-11-09T12:01:04.976594Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vdaoah93 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: bert-base-uncased\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0004682687272526259\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: polynomial\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmustafakeser\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231109_095835-vdaoah93</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/vdaoah93' target=\"_blank\">fancy-sweep-1</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/vdaoah93' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/vdaoah93</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   4 of 4 files downloaded.  \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c5db0a7faa54b3d9a4bc9e76a4751a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a65cc8c05450465da3cdd00d8b3b2947"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bb1ffaa4d3c46ec8fc4bdfc9955ae64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ed1e1b42f0c4c738d1b67bb229f9ed6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa6140c9fced43d1abc0e865ea7c3bdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28bae2b7a5774e5d832a75bc55ce1361"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4a0b9560fe4489789cd47b8d4e700fa"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1790' max='1790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1790/1790 07:27, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.996600</td>\n      <td>1.960535</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.956900</td>\n      <td>1.946229</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.94623</td></tr><tr><td>eval/runtime</td><td>7.4461</td></tr><tr><td>eval/samples_per_second</td><td>53.451</td></tr><tr><td>eval/steps_per_second</td><td>3.357</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>1790</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.9569</td></tr><tr><td>train/total_flos</td><td>1882907237683200.0</td></tr><tr><td>train/train_loss</td><td>1.97678</td></tr><tr><td>train/train_runtime</td><td>449.8171</td></tr><tr><td>train/train_samples_per_second</td><td>15.909</td></tr><tr><td>train/train_steps_per_second</td><td>3.979</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fancy-sweep-1</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/vdaoah93' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/vdaoah93</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231109_095835-vdaoah93/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7ssb6ua3 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: bert-base-uncased\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00012148211314055668\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: polynomial\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231109_100711-7ssb6ua3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/7ssb6ua3' target=\"_blank\">firm-sweep-2</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/7ssb6ua3' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/7ssb6ua3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"767fb66d2fc045d38a81eaa39301ba64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6af7097acbc1479ebd9a0d9b622ca692"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='448' max='448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [448/448 06:40, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.787600</td>\n      <td>1.671198</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.570700</td>\n      <td>1.599512</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.487 MB of 0.487 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e728d619e44741be945d12d422da4054"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.59951</td></tr><tr><td>eval/runtime</td><td>7.4491</td></tr><tr><td>eval/samples_per_second</td><td>53.429</td></tr><tr><td>eval/steps_per_second</td><td>3.356</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>448</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.5707</td></tr><tr><td>train/total_flos</td><td>1882907237683200.0</td></tr><tr><td>train/train_loss</td><td>1.67915</td></tr><tr><td>train/train_runtime</td><td>401.7924</td></tr><tr><td>train/train_samples_per_second</td><td>17.81</td></tr><tr><td>train/train_steps_per_second</td><td>1.115</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">firm-sweep-2</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/7ssb6ua3' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/7ssb6ua3</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231109_100711-7ssb6ua3/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ptvhysk3 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: distilbert-base-uncased\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0015244923145798347\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: cosine\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231109_101443-ptvhysk3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/ptvhysk3' target=\"_blank\">laced-sweep-3</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/ptvhysk3' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/ptvhysk3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6922ba7d748b44c38795feca48c286ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52322189bec44069a1d31f4423643ec6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8acca8c28a8b4b0ca486cd89ef618c11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91308b1e147243d18c909ea2c65ea464"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"531e2226c2cf4f03a545c138e9a8252f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"047c0131e9284dcfb73a8555605f80b3"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e3dda96041d48839a94b5569ffa28ee"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='448' max='448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [448/448 03:29, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.974800</td>\n      <td>1.947357</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.946000</td>\n      <td>1.945910</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.499 MB of 0.499 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25ab198d1d964f0988d3643cbc781aa3"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.94591</td></tr><tr><td>eval/runtime</td><td>3.8752</td></tr><tr><td>eval/samples_per_second</td><td>102.706</td></tr><tr><td>eval/steps_per_second</td><td>6.451</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>448</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.946</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.9604</td></tr><tr><td>train/train_runtime</td><td>209.8327</td></tr><tr><td>train/train_samples_per_second</td><td>34.103</td></tr><tr><td>train/train_steps_per_second</td><td>2.135</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">laced-sweep-3</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/ptvhysk3' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/ptvhysk3</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231109_101443-ptvhysk3/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8y04mgww with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: distilroberta-base\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001684716496566905\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: cosine\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231109_101901-8y04mgww</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/8y04mgww' target=\"_blank\">rural-sweep-4</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/8y04mgww' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/8y04mgww</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49be47258ae4458c81577adfd86ffba5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94eec25957144df793db1fe7b5fb1fd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4adb17c4903841e7b78a4364aa9b491c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"517dd507d0a74cc0a1cb72bdbb6db93c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07e50d6cb7514b3997627697bd03f01d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d03363317faa462782818c5d11acdc6d"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd4809cc5c3c41fd9ff039d03d60e874"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1790' max='1790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1790/1790 04:01, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.975400</td>\n      <td>1.954528</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.953100</td>\n      <td>1.946631</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.94663</td></tr><tr><td>eval/runtime</td><td>3.8906</td></tr><tr><td>eval/samples_per_second</td><td>102.297</td></tr><tr><td>eval/steps_per_second</td><td>6.426</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>1790</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.9531</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.96425</td></tr><tr><td>train/train_runtime</td><td>242.1468</td></tr><tr><td>train/train_samples_per_second</td><td>29.552</td></tr><tr><td>train/train_steps_per_second</td><td>7.392</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">rural-sweep-4</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/8y04mgww' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/8y04mgww</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231109_101901-8y04mgww/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 20bol7cf with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: distilbert-base-uncased\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00015942974320807068\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: polynomial\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231109_102401-20bol7cf</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/20bol7cf' target=\"_blank\">dulcet-sweep-5</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/20bol7cf' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/20bol7cf</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cdecd01c52442e1b162197b8a2e2b97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ff03d5665424786819e3a021f64ebb9"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1790' max='1790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1790/1790 03:57, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.865000</td>\n      <td>2.049907</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.793700</td>\n      <td>1.813560</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.81356</td></tr><tr><td>eval/runtime</td><td>3.8669</td></tr><tr><td>eval/samples_per_second</td><td>102.925</td></tr><tr><td>eval/steps_per_second</td><td>6.465</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>1790</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.7937</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.82936</td></tr><tr><td>train/train_runtime</td><td>238.1211</td></tr><tr><td>train/train_samples_per_second</td><td>30.052</td></tr><tr><td>train/train_steps_per_second</td><td>7.517</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">dulcet-sweep-5</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/20bol7cf' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/20bol7cf</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231109_102401-20bol7cf/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: be9e2x1i with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: distilroberta-base\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00014046399469880863\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: linear\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231109_102852-be9e2x1i</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/be9e2x1i' target=\"_blank\">fragrant-sweep-6</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/be9e2x1i' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/be9e2x1i</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95a0609eca4c4210aa5386989f37b43f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa0b72b225c0414a955e5d2a11d7480a"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='896' max='896' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [896/896 03:39, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.847400</td>\n      <td>1.765050</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.651900</td>\n      <td>1.639143</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.63914</td></tr><tr><td>eval/runtime</td><td>3.8658</td></tr><tr><td>eval/samples_per_second</td><td>102.955</td></tr><tr><td>eval/steps_per_second</td><td>6.467</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>896</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.6519</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.74961</td></tr><tr><td>train/train_runtime</td><td>219.435</td></tr><tr><td>train/train_samples_per_second</td><td>32.611</td></tr><tr><td>train/train_steps_per_second</td><td>4.083</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fragrant-sweep-6</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/be9e2x1i' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/be9e2x1i</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231109_102852-be9e2x1i/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rzsnukwr with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: distilbert-base-uncased\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0013474318112704808\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: polynomial\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231109_103321-rzsnukwr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/rzsnukwr' target=\"_blank\">wobbly-sweep-7</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/rzsnukwr' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/rzsnukwr</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ade18099ce6341d58b096c8b493491d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d1f3dbbab264d599765565d0ac42b1e"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1790' max='1790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1790/1790 03:57, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.971200</td>\n      <td>1.946030</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.946300</td>\n      <td>1.946040</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁█</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.94604</td></tr><tr><td>eval/runtime</td><td>3.8718</td></tr><tr><td>eval/samples_per_second</td><td>102.793</td></tr><tr><td>eval/steps_per_second</td><td>6.457</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>1790</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.9463</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.95871</td></tr><tr><td>train/train_runtime</td><td>237.358</td></tr><tr><td>train/train_samples_per_second</td><td>30.149</td></tr><tr><td>train/train_steps_per_second</td><td>7.541</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">wobbly-sweep-7</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/rzsnukwr' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/rzsnukwr</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231109_103321-rzsnukwr/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: r3zdsacy with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: distilroberta-base\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00033538507484628526\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: cosine\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231109_103811-r3zdsacy</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/r3zdsacy' target=\"_blank\">zesty-sweep-8</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/r3zdsacy' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/r3zdsacy</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8889cc5d260e425798c713897ef3b6e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88ca89eacedb48f5bd337bcf3fbcc5e9"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1790' max='1790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1790/1790 04:01, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.001400</td>\n      <td>1.953425</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.954000</td>\n      <td>1.946548</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.94655</td></tr><tr><td>eval/runtime</td><td>3.866</td></tr><tr><td>eval/samples_per_second</td><td>102.949</td></tr><tr><td>eval/steps_per_second</td><td>6.467</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>1790</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.954</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.9777</td></tr><tr><td>train/train_runtime</td><td>241.8754</td></tr><tr><td>train/train_samples_per_second</td><td>29.585</td></tr><tr><td>train/train_steps_per_second</td><td>7.401</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">zesty-sweep-8</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/r3zdsacy' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/r3zdsacy</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231109_103811-r3zdsacy/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b1wekpie with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: distilroberta-base\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0013634126139094823\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: cosine\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231109_104305-b1wekpie</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/b1wekpie' target=\"_blank\">dandy-sweep-9</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/b1wekpie' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/b1wekpie</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f30f5726b1b4008860ba7c4921c7a87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6238e93761194af68bdeb875bb37aaf1"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1790' max='1790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1790/1790 04:03, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.052600</td>\n      <td>1.949695</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.955900</td>\n      <td>1.946200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.9462</td></tr><tr><td>eval/runtime</td><td>3.8673</td></tr><tr><td>eval/samples_per_second</td><td>102.915</td></tr><tr><td>eval/steps_per_second</td><td>6.465</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>1790</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.9559</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>2.00425</td></tr><tr><td>train/train_runtime</td><td>243.7753</td></tr><tr><td>train/train_samples_per_second</td><td>29.355</td></tr><tr><td>train/train_steps_per_second</td><td>7.343</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">dandy-sweep-9</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/b1wekpie' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/b1wekpie</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231109_104305-b1wekpie/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sd6nabyf with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: bert-base-uncased\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0007781436315843396\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: cosine\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231109_104804-sd6nabyf</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/sd6nabyf' target=\"_blank\">pious-sweep-10</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/sd6nabyf' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/sd6nabyf</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22b7f9089150452b96291c47b74c69fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf5b5aa16ba64befa89278e142b5be74"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='448' max='448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [448/448 06:44, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.992500</td>\n      <td>1.966883</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.952000</td>\n      <td>1.947770</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.556 MB of 0.556 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef3ef2f84814424d9a55c5ba737aeb39"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.94777</td></tr><tr><td>eval/runtime</td><td>7.4461</td></tr><tr><td>eval/samples_per_second</td><td>53.451</td></tr><tr><td>eval/steps_per_second</td><td>3.357</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>448</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.952</td></tr><tr><td>train/total_flos</td><td>1882907237683200.0</td></tr><tr><td>train/train_loss</td><td>1.97222</td></tr><tr><td>train/train_runtime</td><td>404.9638</td></tr><tr><td>train/train_samples_per_second</td><td>17.671</td></tr><tr><td>train/train_steps_per_second</td><td>1.106</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">pious-sweep-10</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/sd6nabyf' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/sd6nabyf</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231109_104804-sd6nabyf/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mmfulxav with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: distilbert-base-uncased\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00020249578309784896\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: linear\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231109_105542-mmfulxav</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/mmfulxav' target=\"_blank\">stoic-sweep-11</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/mmfulxav' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/mmfulxav</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72b033e764064c5e8ff26c5c8a96d87b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84208dbdd38940c78893e09bd383fc9a"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='896' max='896' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [896/896 03:37, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.962500</td>\n      <td>1.950685</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.917100</td>\n      <td>1.854917</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.943 MB of 0.943 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bd2f417aecb4a4996459b324c840021"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.85492</td></tr><tr><td>eval/runtime</td><td>3.8822</td></tr><tr><td>eval/samples_per_second</td><td>102.52</td></tr><tr><td>eval/steps_per_second</td><td>6.44</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>896</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.9171</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.93981</td></tr><tr><td>train/train_runtime</td><td>217.8335</td></tr><tr><td>train/train_samples_per_second</td><td>32.851</td></tr><tr><td>train/train_steps_per_second</td><td>4.113</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">stoic-sweep-11</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/mmfulxav' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/mmfulxav</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231109_105542-mmfulxav/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zddff35t with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: bert-base-uncased\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00025427081951376226\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: linear\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231109_110015-zddff35t</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/zddff35t' target=\"_blank\">silver-sweep-12</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/zddff35t' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/zddff35t</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1633d282a94a496bbc4d710f58d3bdf1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47638272ac3c4b7283035fb0e56f7465"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='448' max='448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [448/448 06:44, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.966800</td>\n      <td>1.947442</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.943600</td>\n      <td>1.937133</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.573 MB of 0.573 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4edd30f76014e84a1c8dbe850f5242c"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.93713</td></tr><tr><td>eval/runtime</td><td>7.464</td></tr><tr><td>eval/samples_per_second</td><td>53.323</td></tr><tr><td>eval/steps_per_second</td><td>3.349</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>448</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.9436</td></tr><tr><td>train/total_flos</td><td>1882907237683200.0</td></tr><tr><td>train/train_loss</td><td>1.95519</td></tr><tr><td>train/train_runtime</td><td>405.1042</td></tr><tr><td>train/train_samples_per_second</td><td>17.665</td></tr><tr><td>train/train_steps_per_second</td><td>1.106</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">silver-sweep-12</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/zddff35t' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/zddff35t</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231109_110015-zddff35t/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dn04bojm with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: distilbert-base-uncased\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0012658039459375705\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: cosine\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231109_110757-dn04bojm</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/dn04bojm' target=\"_blank\">hearty-sweep-13</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/dn04bojm' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/dn04bojm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"931ee8bbcc0f4d8f981bbdecb94bc645"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13a308ba3d514560b8d81a49eafacacc"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='448' max='448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [448/448 03:29, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.971800</td>\n      <td>1.946639</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.948500</td>\n      <td>1.946151</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.94615</td></tr><tr><td>eval/runtime</td><td>3.8812</td></tr><tr><td>eval/samples_per_second</td><td>102.545</td></tr><tr><td>eval/steps_per_second</td><td>6.441</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>448</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.9485</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.96013</td></tr><tr><td>train/train_runtime</td><td>209.9818</td></tr><tr><td>train/train_samples_per_second</td><td>34.079</td></tr><tr><td>train/train_steps_per_second</td><td>2.134</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">hearty-sweep-13</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/dn04bojm' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/dn04bojm</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231109_110757-dn04bojm/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tz4efiup with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: bert-base-uncased\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0004165608711615979\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: cosine\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231109_111215-tz4efiup</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/tz4efiup' target=\"_blank\">glowing-sweep-14</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/tz4efiup' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/tz4efiup</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd4d9155f41048d48303d7a30fb4af98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f826874ac6c34ed1895bba4b807a332e"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1790' max='1790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1790/1790 07:34, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.036700</td>\n      <td>1.961290</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.971000</td>\n      <td>1.947189</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='1.729 MB of 1.729 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15518bf33c93415c9526277951259187"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.94719</td></tr><tr><td>eval/runtime</td><td>7.4394</td></tr><tr><td>eval/samples_per_second</td><td>53.499</td></tr><tr><td>eval/steps_per_second</td><td>3.36</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>1790</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.971</td></tr><tr><td>train/total_flos</td><td>1882907237683200.0</td></tr><tr><td>train/train_loss</td><td>2.00383</td></tr><tr><td>train/train_runtime</td><td>454.5554</td></tr><tr><td>train/train_samples_per_second</td><td>15.743</td></tr><tr><td>train/train_steps_per_second</td><td>3.938</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">glowing-sweep-14</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/tz4efiup' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/tz4efiup</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231109_111215-tz4efiup/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zgqh2338 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: bert-base-uncased\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001034355665955621\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: cosine\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231109_112039-zgqh2338</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/zgqh2338' target=\"_blank\">stilted-sweep-15</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/zgqh2338' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/zgqh2338</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba28bf3a9c2b47a4b5ea581045d4d732"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baf75d976969416d831ab40a03422997"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1790' max='1790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1790/1790 07:28, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.973600</td>\n      <td>1.959510</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.950800</td>\n      <td>1.946805</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.94681</td></tr><tr><td>eval/runtime</td><td>7.4509</td></tr><tr><td>eval/samples_per_second</td><td>53.416</td></tr><tr><td>eval/steps_per_second</td><td>3.355</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>1790</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.9508</td></tr><tr><td>train/total_flos</td><td>1882907237683200.0</td></tr><tr><td>train/train_loss</td><td>1.96222</td></tr><tr><td>train/train_runtime</td><td>448.9253</td></tr><tr><td>train/train_samples_per_second</td><td>15.94</td></tr><tr><td>train/train_steps_per_second</td><td>3.987</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">stilted-sweep-15</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/zgqh2338' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/zgqh2338</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231109_112039-zgqh2338/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 47kg0thg with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: bert-base-uncased\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003932086111349965\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: polynomial\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231109_112900-47kg0thg</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/47kg0thg' target=\"_blank\">restful-sweep-16</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/47kg0thg' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/47kg0thg</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"077d230a36f548ab90b5a16efdbc4d35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6d32e8c66304d6da44132552492c749"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='896' max='896' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [896/896 06:59, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.012000</td>\n      <td>1.961284</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.975500</td>\n      <td>1.946179</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.985 MB of 0.985 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55156b0630df4aac83e9dd575c0a94c9"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.94618</td></tr><tr><td>eval/runtime</td><td>7.4378</td></tr><tr><td>eval/samples_per_second</td><td>53.511</td></tr><tr><td>eval/steps_per_second</td><td>3.361</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>896</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.9755</td></tr><tr><td>train/total_flos</td><td>1882907237683200.0</td></tr><tr><td>train/train_loss</td><td>1.99374</td></tr><tr><td>train/train_runtime</td><td>419.9623</td></tr><tr><td>train/train_samples_per_second</td><td>17.04</td></tr><tr><td>train/train_steps_per_second</td><td>2.134</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">restful-sweep-16</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/47kg0thg' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/47kg0thg</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231109_112900-47kg0thg/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ipkwym4v with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: distilroberta-base\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0010918810064765956\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: polynomial\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231109_113650-ipkwym4v</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/ipkwym4v' target=\"_blank\">visionary-sweep-17</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/ipkwym4v' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/ipkwym4v</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bf5c5287adf4120b4a6b81059557f5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05b621d816834c81ba4f9db4c587de73"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='896' max='896' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [896/896 03:39, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.013700</td>\n      <td>1.948326</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.953400</td>\n      <td>1.946038</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.94604</td></tr><tr><td>eval/runtime</td><td>3.856</td></tr><tr><td>eval/samples_per_second</td><td>103.215</td></tr><tr><td>eval/steps_per_second</td><td>6.483</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>896</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.9534</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.98356</td></tr><tr><td>train/train_runtime</td><td>219.5849</td></tr><tr><td>train/train_samples_per_second</td><td>32.589</td></tr><tr><td>train/train_steps_per_second</td><td>4.08</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">visionary-sweep-17</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/ipkwym4v' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/ipkwym4v</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231109_113650-ipkwym4v/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hiiz0zca with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: distilbert-base-uncased\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003781242878558797\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: linear\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231109_114123-hiiz0zca</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/hiiz0zca' target=\"_blank\">stellar-sweep-18</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/hiiz0zca' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/hiiz0zca</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c473ced449b54193a8159b6ef4afa870"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0aa0075ae274e44b226519f018238ed"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='448' max='448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [448/448 03:29, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.968200</td>\n      <td>1.952204</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.879200</td>\n      <td>1.827658</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.82766</td></tr><tr><td>eval/runtime</td><td>3.9148</td></tr><tr><td>eval/samples_per_second</td><td>101.667</td></tr><tr><td>eval/steps_per_second</td><td>6.386</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>448</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.8792</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.92372</td></tr><tr><td>train/train_runtime</td><td>209.6228</td></tr><tr><td>train/train_samples_per_second</td><td>34.138</td></tr><tr><td>train/train_steps_per_second</td><td>2.137</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">stellar-sweep-18</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/hiiz0zca' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/hiiz0zca</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231109_114123-hiiz0zca/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vonyio99 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: bert-base-uncased\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005873766334117263\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: cosine\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231109_114543-vonyio99</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/vonyio99' target=\"_blank\">glad-sweep-19</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/vonyio99' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/vonyio99</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f4d6c977a074a6fbd68e8000b7f23d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de04c1196ac1453d9830b17eabf4c493"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='448' max='448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [448/448 06:42, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.024400</td>\n      <td>1.980799</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.971300</td>\n      <td>1.947941</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.94794</td></tr><tr><td>eval/runtime</td><td>7.4255</td></tr><tr><td>eval/samples_per_second</td><td>53.599</td></tr><tr><td>eval/steps_per_second</td><td>3.367</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>448</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.9713</td></tr><tr><td>train/total_flos</td><td>1882907237683200.0</td></tr><tr><td>train/train_loss</td><td>1.99784</td></tr><tr><td>train/train_runtime</td><td>403.8352</td></tr><tr><td>train/train_samples_per_second</td><td>17.72</td></tr><tr><td>train/train_steps_per_second</td><td>1.109</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">glad-sweep-19</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/vonyio99' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/vonyio99</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231109_114543-vonyio99/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z01uv9h4 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: bert-base-uncased\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0006328288020660093\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: linear\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231109_115320-z01uv9h4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/z01uv9h4' target=\"_blank\">scarlet-sweep-20</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/q9q9767w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/z01uv9h4' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/z01uv9h4</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ff27dfbebb5464f80f5766d098ad7ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecf7cb646217490a81115ae2b162fe3a"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='896' max='896' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [896/896 06:55, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.033500</td>\n      <td>1.964422</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.981200</td>\n      <td>1.946555</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.94656</td></tr><tr><td>eval/runtime</td><td>7.4523</td></tr><tr><td>eval/samples_per_second</td><td>53.406</td></tr><tr><td>eval/steps_per_second</td><td>3.355</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>896</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.9812</td></tr><tr><td>train/total_flos</td><td>1882907237683200.0</td></tr><tr><td>train/train_loss</td><td>2.00731</td></tr><tr><td>train/train_runtime</td><td>415.7568</td></tr><tr><td>train/train_samples_per_second</td><td>17.212</td></tr><tr><td>train/train_steps_per_second</td><td>2.155</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">scarlet-sweep-20</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/z01uv9h4' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/z01uv9h4</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231109_115320-z01uv9h4/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2023-11-09T12:10:32.225088Z","iopub.execute_input":"2023-11-09T12:10:32.225486Z","iopub.status.idle":"2023-11-09T12:10:32.230160Z","shell.execute_reply.started":"2023-11-09T12:10:32.225457Z","shell.execute_reply":"2023-11-09T12:10:32.228991Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#1.5 h p100","metadata":{},"execution_count":null,"outputs":[]}]}