{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style=\"border: 2px solid #555; color:black; border-radius: 10px; background-color: #0074D9; padding: 10px; font-size: 20px; text-align: center;\">Introduction</div>","metadata":{}},{"cell_type":"markdown","source":"**Table Of Content:**\n* [Introduction](#1)\n* [Refactor and Define utils](#2)\n* [Refactor Train](#3)\n* [Sweeps](#4)\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# <div style=\"border: 2px solid #555; color:black; border-radius: 10px; background-color: #0074D9; padding: 10px; font-size: 20px; text-align: center;\">Refactor and Define utils</div>\n* [return top](#1)","metadata":{}},{"cell_type":"code","source":"%%writefile utils.py\nimport json\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport torch\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer, EarlyStoppingCallback\nimport torch.nn.functional as F\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport torch\nfrom datasets import Dataset\nimport io\n\ndef read_data():\n    \"\"\"\n    return train , test\n    \"\"\"\n    with open(\"/kaggle/working/artifacts/detect_llm_raw_data:v1/train_df.table.json\") as json_data:\n        data = json.load(json_data)\n        train = pd.DataFrame(data = data[\"data\"],columns=data[\"columns\"])\n        json_data.close()\n\n    with open(\"/kaggle/working/artifacts/detect_llm_raw_data:v1/test_df.table.json\") as json_data:\n        data = json.load(json_data)\n        test = pd.DataFrame(data = data[\"data\"],columns=data[\"columns\"])\n        json_data.close()\n    return train , test\n\ndef preprocess(train=None,test=None):\n    \"\"\"\n    return dataset_train, dataset_test\n    \"\"\"\n    train.fillna(\" \",inplace=True)\n    test.fillna(\" \",inplace=True)\n    train[\"text\"] = train[\"Question\"] + \" \" + train[\"Response\"]\n    test[\"text\"] = test[\"Question\"] + \" \" + test[\"Response\"]\n    df_train = train[[\"target\",\"text\"]]\n    df_test = test[[\"text\"]]\n    dataset_train = Dataset.from_pandas(df_train)\n    dataset_test = Dataset.from_pandas(df_test)\n    \n    return dataset_train, dataset_test\n\n\ndef dataset_tokenize_n_split(train, dataset_train, dataset_test):\n    \"\"\"\n    return split_train_dataset,split_eval_dataset , tokenized_test , tokenizer\n    \"\"\"\n    tokenizer       = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n    def tokenize_function(examples):\n    \n        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n    tokenized_train = dataset_train.map(tokenize_function, batched=True)\n    tokenized_test  = dataset_test.map(tokenize_function, batched=True)\n    tokenized_train = tokenized_train.remove_columns(['text'])\n    tokenized_train = tokenized_train.rename_column(\"target\", \"labels\")\n    tokenized_test = tokenized_test.remove_columns(['text'])\n\n    kf= StratifiedKFold(n_splits=10,shuffle=True,random_state=42)\n    for i , (tr_idx,val_idx) in enumerate(kf.split(train,train.target)):\n        print(f\"Fold : {i}\")\n        print(f\"shape train : {tr_idx.shape}\")\n        print(f\"shape val : {val_idx.shape}\")\n        break\n        \n    \n    split_train_dataset = tokenized_train.select(tr_idx)\n    split_eval_dataset = tokenized_train.select(val_idx)\n\n    return split_train_dataset,split_eval_dataset , tokenized_test , tokenizer\n\ndef predict_fn(dataset_ = None):\n    \n    \"\"\"\n    return mean of all_probabilities (m,7)\n    \"\"\"\n    input_ids = dataset_['input_ids']\n    # token_type_ids = dataset_['token_type_ids']\n    attention_mask = dataset_['attention_mask']\n\n    # Move the input tensors to the GPU\n    input_ids = torch.tensor(input_ids).to('cuda:0')\n    # token_type_ids = torch.tensor(token_type_ids).to('cuda:0')\n    attention_mask = torch.tensor(attention_mask).to('cuda:0')\n\n    # Define batch size\n    batch_size = 8\n\n    # Calculate the number of batches\n    num_samples = len(input_ids)\n    num_batches = (num_samples + batch_size - 1) // batch_size\n\n    # Initialize a list to store the softmax probabilities\n    all_probabilities = []\n\n    # Make predictions in batches\n    with torch.no_grad():\n        for batch in range(num_batches):\n            start_idx = batch * batch_size\n            end_idx = min((batch + 1) * batch_size, num_samples)\n\n            batch_input_ids = input_ids[start_idx:end_idx]\n    #         batch_token_type_ids = token_type_ids[start_idx:end_idx]\n            batch_attention_mask = attention_mask[start_idx:end_idx]\n\n            outputs = model(input_ids=batch_input_ids, \n    #                         token_type_ids=batch_token_type_ids, \n                            attention_mask=batch_attention_mask)\n            logits = outputs.logits\n\n            # Apply softmax to get probabilities\n            probabilities = F.softmax(logits, dim=1)\n\n\n            all_probabilities.extend(probabilities.tolist())\n    return np.concatenate(all_probabilities,axis=0).reshape(dataset_.shape[0],7)\n\n\ndef conf_mat(df_val = None,preds_val = None):\n    \"\"\"\n    no return\n    \"\"\"\n    plt.figure(figsize=(8,8))\n    ConfusionMatrixDisplay.from_predictions(df_val.target,np.argmax(preds_val,axis=1))\n    plt.savefig(f\"val_conf_matrix.png\", format=\"png\")\n    plt.show();\n    conf = wandb.Image(data_or_path=\"val_conf_matrix.png\")\n    wandb.log({\"val_conf_matrix\": conf})\ndef create_model(model_name = \"distilroberta-base\",num_labels = 7):\n    \"\"\"\n    return\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n    # Specify the GPU device\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    # Move your model to the GPU\n    model.to(device);\n    \n    return model\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-10-28T13:12:13.169159Z","iopub.execute_input":"2023-10-28T13:12:13.169765Z","iopub.status.idle":"2023-10-28T13:12:13.181411Z","shell.execute_reply.started":"2023-10-28T13:12:13.169733Z","shell.execute_reply":"2023-10-28T13:12:13.180501Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Writing utils.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# <div style=\"border: 2px solid #555; color:black; border-radius: 10px; background-color: #0074D9; padding: 10px; font-size: 20px; text-align: center;\">Refactor Train</div>\n* [return top](#1)","metadata":{}},{"cell_type":"code","source":"# %%writefile train.py\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport torch\nfrom datasets import Dataset\nimport json\nfrom IPython.display import display\nimport wandb\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nfrom transformers import AutoModelForSequenceClassification,TrainerCallback\nfrom transformers import TrainingArguments, Trainer, EarlyStoppingCallback\nimport torch.nn.functional as F\nfrom utils import *\nimport io\n\nclass WandbMetricsLogger(TrainerCallback):\n    def on_evaluate(self, args, state, control, model, metrics):\n        # Log metrics to Wandb\n        wandb.log(metrics)\n        \ndefault_config = {\n        'method': 'random',\n        'metric': {\n        'goal': 'minimize', \n        'name': 'eval_loss'\n        },\n    }\n\n\n    # hyperparameters\nparameters_dict = {\n        'epochs': {\n            'value': 2\n            },\n        'seed': {\n            'value': 42\n            },\n        'batch_size': {\n            'values': [8, 16, 32]\n            },\n        'learning_rate': {\n            'distribution': 'log_uniform_values',\n            'min': 1e-5,\n            'max': 1e-3\n        },\n        'weight_decay': {\n            'values': [0.0, 0.2]\n        },\n        'learning_sch': {\n            'values': ['linear','polynomial','cosine']\n        },\n    }\n\n\ndefault_config['parameters'] = parameters_dict\n\ndef compute_metrics_fn(eval_preds):\n    metrics = dict()\n\n    # Extract the validation loss from eval_preds\n    validation_loss = eval_preds.loss\n    metrics['validation_loss'] = validation_loss\n\n    return metrics\n\ndef parse_args():\n    \"Overriding default argments\"\n    argparser = argparse.ArgumentParser(description='Process hyper-parameters')\n    argparser.add_argument('--batch_size', type=int, default=default_config.get(\"parameters\").get(\"batch_size\").get(\"values\")[-1],\n                           help='batch size')\n    argparser.add_argument('--epochs', type=int, default=default_config.get(\"parameters\").get(\"epochs\").get(\"value\"),\n                           help='number of training epochs')\n    argparser.add_argument('--lr', type=float, default=default_config.get(\"parameters\").get(\"learning_rate\").get(\"min\"),\n                           help='learning rate')\n    argparser.add_argument('--seed', type=int, default=default_config.get(\"parameters\").get(\"seed\").get(\"value\"),\n                           help='random seed')\n    argparser.add_argument('--weight_decay', type=float, default=default_config.get(\"parameters\").get(\"weight_decay\").get(\"values\")[-1],\n                           help='random seed')\n    \n    args = argparser.parse_args()\n    vars(default_config).update(vars(args))\n    return\n\n\n\ndef train(config=None):\n    \n    torch.manual_seed(default_config.get(\"parameters\").get(\"seed\").get(\"value\"))\n    \n    run = wandb.init(\n                project=\"h2o-ai-predict-the-llm-kaggle-competition\", \n                entity=None, \n                   job_type=\"hyperparameter-tuning\"\n    )\n    if \"artifacts\" not in os.listdir():\n        raw_data_at = run.use_artifact('mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/detect_llm_raw_data:v1', \n                                                       type='raw_data')\n        artifact_di = raw_data_at.download()\n    else: pass\n    train , test = read_data()\n    dataset_train, dataset_test = preprocess(train=train,test=test)\n    split_train_dataset,split_eval_dataset , tokenized_test , tokenizer = dataset_tokenize_n_split(train,dataset_train, dataset_test)\n\n    \n    config = wandb.config\n    \n    model = create_model(model_name = \"distilroberta-base\",num_labels = 7)\n    \n    num_train_epochs=2.\n    training_args = TrainingArguments(                                \n\n                                output_dir='h2o-ai-sweeps',\n                                report_to='wandb',  # Turn on Weights & Biases logging\n                                num_train_epochs=config.epochs,\n                                learning_rate=config.learning_rate,\n                                lr_scheduler_type = config.learning_sch,\n                                per_device_train_batch_size=config.batch_size,\n                                per_device_eval_batch_size=16,\n                                save_strategy='epoch',\n                                evaluation_strategy='epoch',\n                                logging_strategy='epoch',\n                                metric_for_best_model=\"eval_loss\", \n                                load_best_model_at_end=True,\n                                remove_unused_columns=False,\n                                greater_is_better=False\n                                \n\n                                 )\n    early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n    trainer = Trainer(\n                        model=model,\n                        args=training_args,\n                        train_dataset=split_train_dataset,\n                        eval_dataset=split_eval_dataset,\n                        callbacks = [early_stopping],\n                        tokenizer=tokenizer,\n        )\n    trainer.train()\n\n    \n# if __name__==\"__main__\":\n# #     wandb.agent(sweep_id, train, count=20)\n#     parse_args()\n#     train(default_config)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T13:12:13.242878Z","iopub.execute_input":"2023-10-28T13:12:13.243222Z","iopub.status.idle":"2023-10-28T13:12:27.452075Z","shell.execute_reply.started":"2023-10-28T13:12:13.243194Z","shell.execute_reply":"2023-10-28T13:12:27.451010Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# <div style=\"border: 2px solid #555; color:black; border-radius: 10px; background-color: #0074D9; padding: 10px; font-size: 20px; text-align: center;\">Sweeps</div>\n* [return top](#1)","metadata":{}},{"cell_type":"code","source":"wandb.login(relogin=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T13:12:27.453802Z","iopub.execute_input":"2023-10-28T13:12:27.454391Z","iopub.status.idle":"2023-10-28T13:12:35.648188Z","shell.execute_reply.started":"2023-10-28T13:12:27.454362Z","shell.execute_reply":"2023-10-28T13:12:35.647251Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"sweep_id = wandb.sweep(default_config, project='h2o-ai-predict-the-llm-kaggle-competition')","metadata":{"execution":{"iopub.status.busy":"2023-10-28T13:12:39.806083Z","iopub.execute_input":"2023-10-28T13:12:39.806433Z","iopub.status.idle":"2023-10-28T13:12:41.513618Z","shell.execute_reply.started":"2023-10-28T13:12:39.806407Z","shell.execute_reply":"2023-10-28T13:12:41.512732Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Create sweep with ID: dp0ecg1w\nSweep URL: https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.agent(sweep_id, train, count=20)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T13:12:44.128918Z","iopub.execute_input":"2023-10-28T13:12:44.129302Z","iopub.status.idle":"2023-10-28T14:40:02.765897Z","shell.execute_reply.started":"2023-10-28T13:12:44.129274Z","shell.execute_reply":"2023-10-28T14:40:02.764854Z"},"scrolled":true,"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3j17rpcc with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5.5765535179451664e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: cosine\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmustafakeser\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231028_131247-3j17rpcc</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/3j17rpcc' target=\"_blank\">chocolate-sweep-1</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/3j17rpcc' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/3j17rpcc</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   4 of 4 files downloaded.  \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba0b4ba7d05d49c19fdee72d2ecc2e87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07906a8ba3134991b1126ac0b8e0bd69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af94a6b13ecf44289ef484cde7610620"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03c04b6c41164032920075571bd6ee70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49ed80fbd12e4190a24e4b6fe6c2b8be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cc7be46fa104eedb2aa68c8c5dc8a12"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4ae7561c1a74bdcb30f28d6d583643b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"045f644933dc4c039541d6ac4f97c7af"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='224' max='224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [224/224 03:22, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.810800</td>\n      <td>1.733616</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.694200</td>\n      <td>1.712880</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.295 MB of 0.295 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3bd3b7b63434dccacff636de1afcd6a"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.71288</td></tr><tr><td>eval/runtime</td><td>3.9109</td></tr><tr><td>eval/samples_per_second</td><td>101.766</td></tr><tr><td>eval/steps_per_second</td><td>6.392</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>224</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.6942</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.75249</td></tr><tr><td>train/train_runtime</td><td>205.7112</td></tr><tr><td>train/train_samples_per_second</td><td>34.787</td></tr><tr><td>train/train_steps_per_second</td><td>1.089</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">chocolate-sweep-1</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/3j17rpcc' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/3j17rpcc</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231028_131247-3j17rpcc/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mcahupjl with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 4.818258016086e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: linear\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231028_131710-mcahupjl</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/mcahupjl' target=\"_blank\">efficient-sweep-2</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/mcahupjl' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/mcahupjl</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e710cd924ae4b7f8b8f6ac76d0b9d63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f9d44268a3d442580a97a607d393d6a"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='896' max='896' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [896/896 03:39, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.829500</td>\n      <td>1.779587</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.745900</td>\n      <td>1.728652</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.869 MB of 0.869 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3b3ad6983d349f1b64ecc90b0fcacd1"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.72865</td></tr><tr><td>eval/runtime</td><td>3.9221</td></tr><tr><td>eval/samples_per_second</td><td>101.476</td></tr><tr><td>eval/steps_per_second</td><td>6.374</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>896</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.7459</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.7877</td></tr><tr><td>train/train_runtime</td><td>219.6415</td></tr><tr><td>train/train_samples_per_second</td><td>32.58</td></tr><tr><td>train/train_steps_per_second</td><td>4.079</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">efficient-sweep-2</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/mcahupjl' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/mcahupjl</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231028_131710-mcahupjl/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yhprbtzy with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0007180182112163355\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: polynomial\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231028_132139-yhprbtzy</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/yhprbtzy' target=\"_blank\">sparkling-sweep-3</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/yhprbtzy' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/yhprbtzy</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38e9507769394712bf9a0609881b96ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2aebff97915a4b6483f4c7c8e3f4b597"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='224' max='224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [224/224 03:25, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.988100</td>\n      <td>1.953655</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.955300</td>\n      <td>1.947281</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.94728</td></tr><tr><td>eval/runtime</td><td>3.9169</td></tr><tr><td>eval/samples_per_second</td><td>101.611</td></tr><tr><td>eval/steps_per_second</td><td>6.383</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>224</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.9553</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.97168</td></tr><tr><td>train/train_runtime</td><td>206.6019</td></tr><tr><td>train/train_samples_per_second</td><td>34.637</td></tr><tr><td>train/train_steps_per_second</td><td>1.084</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">sparkling-sweep-3</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/yhprbtzy' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/yhprbtzy</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231028_132139-yhprbtzy/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5xe7l8y2 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3.613467348240049e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: cosine\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231028_132552-5xe7l8y2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/5xe7l8y2' target=\"_blank\">misty-sweep-4</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/5xe7l8y2' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/5xe7l8y2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e5581b25fae49dba2edc3f97ebbfe49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14f44bb3ccb2493fb855b53a055bfb1c"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='224' max='224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [224/224 03:25, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.808600</td>\n      <td>1.738096</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.699300</td>\n      <td>1.718404</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.318 MB of 0.318 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e49c3b9031024f56b7056298cee467eb"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.7184</td></tr><tr><td>eval/runtime</td><td>3.9166</td></tr><tr><td>eval/samples_per_second</td><td>101.618</td></tr><tr><td>eval/steps_per_second</td><td>6.383</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>224</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.6993</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.75394</td></tr><tr><td>train/train_runtime</td><td>206.1808</td></tr><tr><td>train/train_samples_per_second</td><td>34.707</td></tr><tr><td>train/train_steps_per_second</td><td>1.086</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">misty-sweep-4</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/5xe7l8y2' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/5xe7l8y2</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231028_132552-5xe7l8y2/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1gz2izaq with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.000883799881461311\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: linear\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231028_133006-1gz2izaq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/1gz2izaq' target=\"_blank\">legendary-sweep-5</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/1gz2izaq' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/1gz2izaq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51846fef10734945bab44c120e1b9926"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d347b46dba749c48d2248048e41a033"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='448' max='448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [448/448 03:33, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.999200</td>\n      <td>1.960397</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.957400</td>\n      <td>1.946956</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.516 MB of 0.516 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cd977ec85924cd9bf9251e83e8c6e19"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.94696</td></tr><tr><td>eval/runtime</td><td>3.9296</td></tr><tr><td>eval/samples_per_second</td><td>101.284</td></tr><tr><td>eval/steps_per_second</td><td>6.362</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>448</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.9574</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.97833</td></tr><tr><td>train/train_runtime</td><td>214.5169</td></tr><tr><td>train/train_samples_per_second</td><td>33.359</td></tr><tr><td>train/train_steps_per_second</td><td>2.088</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">legendary-sweep-5</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/1gz2izaq' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/1gz2izaq</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231028_133006-1gz2izaq/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9ecft6ia with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 6.354784195992922e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: linear\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231028_133429-9ecft6ia</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/9ecft6ia' target=\"_blank\">colorful-sweep-6</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/9ecft6ia' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/9ecft6ia</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a706d91e8e54caa8ecdc29c43902257"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2084f72126fd43ea89e180f21b0df1b1"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='224' max='224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [224/224 03:25, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.809300</td>\n      <td>1.731341</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.695200</td>\n      <td>1.706076</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.335 MB of 0.335 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cc78ddf58e74d4a8fd15a6f615e00a3"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.70608</td></tr><tr><td>eval/runtime</td><td>3.9143</td></tr><tr><td>eval/samples_per_second</td><td>101.679</td></tr><tr><td>eval/steps_per_second</td><td>6.387</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>224</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.6952</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.75223</td></tr><tr><td>train/train_runtime</td><td>206.3919</td></tr><tr><td>train/train_samples_per_second</td><td>34.672</td></tr><tr><td>train/train_steps_per_second</td><td>1.085</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">colorful-sweep-6</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/9ecft6ia' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/9ecft6ia</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231028_133429-9ecft6ia/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uxn4s3op with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3.392354976544033e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: cosine\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231028_133843-uxn4s3op</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/uxn4s3op' target=\"_blank\">wobbly-sweep-7</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/uxn4s3op' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/uxn4s3op</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6c86f0929de4422bb874dbd9e4fe8b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9f29051cc984101bc7c23200843b92a"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='896' max='896' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [896/896 03:39, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.815600</td>\n      <td>1.719137</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.689300</td>\n      <td>1.712819</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.911 MB of 0.911 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"850ed2526b9b431db52ab3dfe446b7a5"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.71282</td></tr><tr><td>eval/runtime</td><td>3.9174</td></tr><tr><td>eval/samples_per_second</td><td>101.598</td></tr><tr><td>eval/steps_per_second</td><td>6.382</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>896</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.6893</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.75247</td></tr><tr><td>train/train_runtime</td><td>220.0085</td></tr><tr><td>train/train_samples_per_second</td><td>32.526</td></tr><tr><td>train/train_steps_per_second</td><td>4.073</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">wobbly-sweep-7</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/uxn4s3op' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/uxn4s3op</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231028_133843-uxn4s3op/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xch9a82b with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3.658860795433211e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: polynomial\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231028_134312-xch9a82b</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/xch9a82b' target=\"_blank\">dark-sweep-8</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/xch9a82b' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/xch9a82b</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ff406b3b9c64bd4b06263d76b854df9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a2e4d80c2484173b3a7a23bd64a356f"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='896' max='896' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [896/896 03:40, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.812000</td>\n      <td>1.745455</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.698700</td>\n      <td>1.686643</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.919 MB of 0.919 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0582b428945b4f7caace57199e199f8d"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.68664</td></tr><tr><td>eval/runtime</td><td>3.9305</td></tr><tr><td>eval/samples_per_second</td><td>101.258</td></tr><tr><td>eval/steps_per_second</td><td>6.36</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>896</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.6987</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.75536</td></tr><tr><td>train/train_runtime</td><td>220.3859</td></tr><tr><td>train/train_samples_per_second</td><td>32.47</td></tr><tr><td>train/train_steps_per_second</td><td>4.066</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">dark-sweep-8</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/xch9a82b' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/xch9a82b</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231028_134312-xch9a82b/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bzrxi4c1 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.73594255725719e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: polynomial\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231028_134740-bzrxi4c1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/bzrxi4c1' target=\"_blank\">hearty-sweep-9</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/bzrxi4c1' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/bzrxi4c1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1c9b99de64248eb9061de2b3ea9ab42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f00b9e11ea041b48a2cf733bd42a7ed"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='448' max='448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [448/448 03:34, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.804300</td>\n      <td>1.747561</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.711600</td>\n      <td>1.711180</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.549 MB of 0.549 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a94249dd42da4306b4f3a05744ecf491"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.71118</td></tr><tr><td>eval/runtime</td><td>3.927</td></tr><tr><td>eval/samples_per_second</td><td>101.349</td></tr><tr><td>eval/steps_per_second</td><td>6.366</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>448</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.7116</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.75797</td></tr><tr><td>train/train_runtime</td><td>214.7942</td></tr><tr><td>train/train_samples_per_second</td><td>33.316</td></tr><tr><td>train/train_steps_per_second</td><td>2.086</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">hearty-sweep-9</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/bzrxi4c1' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/bzrxi4c1</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231028_134740-bzrxi4c1/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8n2vlswm with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003113680093197998\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: linear\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231028_135203-8n2vlswm</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/8n2vlswm' target=\"_blank\">sparkling-sweep-10</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/8n2vlswm' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/8n2vlswm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9fb9c2ecd424c06823b0723400d68d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f53bfade690e4714a33323764acc624f"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='224' max='224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [224/224 03:25, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.874900</td>\n      <td>1.778311</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.772700</td>\n      <td>1.804582</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁█</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.80458</td></tr><tr><td>eval/runtime</td><td>3.9084</td></tr><tr><td>eval/samples_per_second</td><td>101.833</td></tr><tr><td>eval/steps_per_second</td><td>6.397</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>224</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.7727</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.82384</td></tr><tr><td>train/train_runtime</td><td>206.722</td></tr><tr><td>train/train_samples_per_second</td><td>34.617</td></tr><tr><td>train/train_steps_per_second</td><td>1.084</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">sparkling-sweep-10</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/8n2vlswm' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/8n2vlswm</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231028_135203-8n2vlswm/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j1o5u48j with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001268305358453921\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: linear\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231028_135617-j1o5u48j</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/j1o5u48j' target=\"_blank\">blooming-sweep-11</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/j1o5u48j' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/j1o5u48j</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24f457c68bee4ac6be2fae4c05c51830"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b9e9bafbd394b49873c4720c5e6a4bf"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='448' max='448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [448/448 03:34, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.866000</td>\n      <td>1.814876</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.747200</td>\n      <td>1.724897</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.566 MB of 0.566 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a519f3c7603453f8e269c95e0388caf"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.7249</td></tr><tr><td>eval/runtime</td><td>3.9117</td></tr><tr><td>eval/samples_per_second</td><td>101.747</td></tr><tr><td>eval/steps_per_second</td><td>6.391</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>448</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.7472</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.80661</td></tr><tr><td>train/train_runtime</td><td>214.904</td></tr><tr><td>train/train_samples_per_second</td><td>33.299</td></tr><tr><td>train/train_steps_per_second</td><td>2.085</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">blooming-sweep-11</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/j1o5u48j' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/j1o5u48j</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231028_135617-j1o5u48j/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8pto8bup with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0002858764766373609\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: polynomial\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231028_140040-8pto8bup</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/8pto8bup' target=\"_blank\">sparkling-sweep-12</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/8pto8bup' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/8pto8bup</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45da751e9f294787a6066e0cceb5acd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ffe113e9bae4d8ca7b93d46459c21dd"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='896' max='896' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [896/896 03:40, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.989100</td>\n      <td>1.951397</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.956100</td>\n      <td>1.946564</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.950 MB of 0.950 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec70af298ecd4e90bdbde4edc9644998"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.94656</td></tr><tr><td>eval/runtime</td><td>3.9491</td></tr><tr><td>eval/samples_per_second</td><td>100.782</td></tr><tr><td>eval/steps_per_second</td><td>6.331</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>896</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.9561</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.97262</td></tr><tr><td>train/train_runtime</td><td>220.5324</td></tr><tr><td>train/train_samples_per_second</td><td>32.449</td></tr><tr><td>train/train_steps_per_second</td><td>4.063</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">sparkling-sweep-12</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/8pto8bup' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/8pto8bup</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231028_140040-8pto8bup/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: npkw9qvq with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001373681713207237\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: linear\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231028_140515-npkw9qvq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/npkw9qvq' target=\"_blank\">pious-sweep-13</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/npkw9qvq' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/npkw9qvq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79102dd3aef340e98c16f2474b4abf74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51a67aa4cbec4ee7998ceee7bf175b3e"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='896' max='896' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [896/896 03:39, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.837100</td>\n      <td>1.757713</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.709500</td>\n      <td>1.703283</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.959 MB of 0.959 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8c9d9c16fcc45ba82e3ac6b979160da"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.70328</td></tr><tr><td>eval/runtime</td><td>3.9132</td></tr><tr><td>eval/samples_per_second</td><td>101.708</td></tr><tr><td>eval/steps_per_second</td><td>6.389</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>896</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.7095</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.77333</td></tr><tr><td>train/train_runtime</td><td>219.7772</td></tr><tr><td>train/train_samples_per_second</td><td>32.56</td></tr><tr><td>train/train_steps_per_second</td><td>4.077</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">pious-sweep-13</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/npkw9qvq' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/npkw9qvq</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231028_140515-npkw9qvq/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3izh6jy3 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2.3141515315680035e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: polynomial\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231028_140943-3izh6jy3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/3izh6jy3' target=\"_blank\">major-sweep-14</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/3izh6jy3' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/3izh6jy3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcb66934a1be4973b4d8bc748e73038c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8484f43ef2b54d87ad2a5bc4dde21728"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='896' max='896' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [896/896 03:39, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.805600</td>\n      <td>1.752494</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.687100</td>\n      <td>1.690626</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.69063</td></tr><tr><td>eval/runtime</td><td>3.9045</td></tr><tr><td>eval/samples_per_second</td><td>101.934</td></tr><tr><td>eval/steps_per_second</td><td>6.403</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>896</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.6871</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.74637</td></tr><tr><td>train/train_runtime</td><td>220.072</td></tr><tr><td>train/train_samples_per_second</td><td>32.517</td></tr><tr><td>train/train_steps_per_second</td><td>4.071</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">major-sweep-14</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/3izh6jy3' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/3izh6jy3</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231028_140943-3izh6jy3/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 38udvcbv with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0008958315669213104\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: cosine\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231028_141414-38udvcbv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/38udvcbv' target=\"_blank\">crimson-sweep-15</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/38udvcbv' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/38udvcbv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66e71579e14d4f4e8e412ad6a072348e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ff9aec3444a4c588f015fbd875e0b3d"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='224' max='224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [224/224 03:24, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.992900</td>\n      <td>1.955834</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.954900</td>\n      <td>1.947726</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.408 MB of 0.408 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24f26dc18b1d4d6ea79d229c55b479a1"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.94773</td></tr><tr><td>eval/runtime</td><td>3.9042</td></tr><tr><td>eval/samples_per_second</td><td>101.941</td></tr><tr><td>eval/steps_per_second</td><td>6.403</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>224</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.9549</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.97391</td></tr><tr><td>train/train_runtime</td><td>205.8923</td></tr><tr><td>train/train_samples_per_second</td><td>34.756</td></tr><tr><td>train/train_steps_per_second</td><td>1.088</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">crimson-sweep-15</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/38udvcbv' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/38udvcbv</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231028_141414-38udvcbv/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kdokv1t8 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00011703718540656912\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: linear\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231028_141827-kdokv1t8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/kdokv1t8' target=\"_blank\">solar-sweep-16</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/kdokv1t8' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/kdokv1t8</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b0999fb9d9341d3a7c8aedac43d90db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"103a89a2268b4638b7de80937ede3144"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='224' max='224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [224/224 03:24, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.816900</td>\n      <td>1.722703</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.694000</td>\n      <td>1.705139</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.70514</td></tr><tr><td>eval/runtime</td><td>3.9101</td></tr><tr><td>eval/samples_per_second</td><td>101.789</td></tr><tr><td>eval/steps_per_second</td><td>6.394</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>224</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.694</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.75547</td></tr><tr><td>train/train_runtime</td><td>205.6667</td></tr><tr><td>train/train_samples_per_second</td><td>34.794</td></tr><tr><td>train/train_steps_per_second</td><td>1.089</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">solar-sweep-16</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/kdokv1t8' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/kdokv1t8</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231028_141827-kdokv1t8/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: knxqbk1g with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.0118363981607169e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: polynomial\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231028_142241-knxqbk1g</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/knxqbk1g' target=\"_blank\">fallen-sweep-17</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/knxqbk1g' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/knxqbk1g</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d19066cd6f894761b728872faa479ff9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7086a7377c06453282e889a43f0ed155"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='896' max='896' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [896/896 03:39, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.808200</td>\n      <td>1.752365</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.720300</td>\n      <td>1.721905</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.992 MB of 0.992 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5678cf664148438ca6a85734cc1621b3"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.7219</td></tr><tr><td>eval/runtime</td><td>3.9029</td></tr><tr><td>eval/samples_per_second</td><td>101.975</td></tr><tr><td>eval/steps_per_second</td><td>6.405</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>896</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.7203</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.76422</td></tr><tr><td>train/train_runtime</td><td>219.5273</td></tr><tr><td>train/train_samples_per_second</td><td>32.597</td></tr><tr><td>train/train_steps_per_second</td><td>4.081</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fallen-sweep-17</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/knxqbk1g' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/knxqbk1g</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231028_142241-knxqbk1g/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8s7z07t1 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00015259665805831906\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: polynomial\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231028_142709-8s7z07t1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/8s7z07t1' target=\"_blank\">fancy-sweep-18</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/8s7z07t1' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/8s7z07t1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1f62a9348934287be335a0bea9183f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55eb60a49eab42b4b65824b460108300"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='224' max='224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [224/224 03:25, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.806800</td>\n      <td>1.768535</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.683600</td>\n      <td>1.698527</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.69853</td></tr><tr><td>eval/runtime</td><td>3.9591</td></tr><tr><td>eval/samples_per_second</td><td>100.529</td></tr><tr><td>eval/steps_per_second</td><td>6.315</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>224</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.6836</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.74519</td></tr><tr><td>train/train_runtime</td><td>206.6438</td></tr><tr><td>train/train_samples_per_second</td><td>34.63</td></tr><tr><td>train/train_steps_per_second</td><td>1.084</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fancy-sweep-18</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/8s7z07t1' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/8s7z07t1</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231028_142709-8s7z07t1/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4dqlaevr with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2.216965480387029e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: linear\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231028_143123-4dqlaevr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/4dqlaevr' target=\"_blank\">vocal-sweep-19</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/4dqlaevr' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/4dqlaevr</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f176069d474a41119054bab4179c46dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edd30d53379e4b19983a83aa6996c9f8"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='448' max='448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [448/448 03:34, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.799900</td>\n      <td>1.752092</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.711200</td>\n      <td>1.712424</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.631 MB of 0.631 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f985c181e8394a399157d1d76b8afd45"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.71242</td></tr><tr><td>eval/runtime</td><td>3.925</td></tr><tr><td>eval/samples_per_second</td><td>101.4</td></tr><tr><td>eval/steps_per_second</td><td>6.369</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>448</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.7112</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.75556</td></tr><tr><td>train/train_runtime</td><td>215.3109</td></tr><tr><td>train/train_samples_per_second</td><td>33.236</td></tr><tr><td>train/train_steps_per_second</td><td>2.081</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">vocal-sweep-19</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/4dqlaevr' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/4dqlaevr</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231028_143123-4dqlaevr/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xu3d50ex with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0002243360387687802\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_sch: linear\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231028_143547-xu3d50ex</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/xu3d50ex' target=\"_blank\">ruby-sweep-20</a></strong> to <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/sweeps/dp0ecg1w</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/xu3d50ex' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/xu3d50ex</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe2b0243968b40bd9460081dc19664e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebb79e21b459435c907933f650450d81"}},"metadata":{}},{"name":"stdout","text":"Fold : 0\nshape train : (3578,)\nshape val : (398,)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='448' max='448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [448/448 03:34, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.874800</td>\n      <td>1.813246</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.821100</td>\n      <td>1.790207</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.639 MB of 0.639 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"047bd115ed974f53a450edd3b9c5af79"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁███</td></tr><tr><td>train/global_step</td><td>▁▁███</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.79021</td></tr><tr><td>eval/runtime</td><td>3.911</td></tr><tr><td>eval/samples_per_second</td><td>101.765</td></tr><tr><td>eval/steps_per_second</td><td>6.392</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>448</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.8211</td></tr><tr><td>train/total_flos</td><td>948021230309376.0</td></tr><tr><td>train/train_loss</td><td>1.84793</td></tr><tr><td>train/train_runtime</td><td>214.5155</td></tr><tr><td>train/train_samples_per_second</td><td>33.359</td></tr><tr><td>train/train_steps_per_second</td><td>2.088</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">ruby-sweep-20</strong> at: <a href='https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/xu3d50ex' target=\"_blank\">https://wandb.ai/mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/runs/xu3d50ex</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231028_143547-xu3d50ex/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T14:43:51.349463Z","iopub.execute_input":"2023-10-28T14:43:51.349840Z","iopub.status.idle":"2023-10-28T14:43:51.354350Z","shell.execute_reply.started":"2023-10-28T14:43:51.349812Z","shell.execute_reply":"2023-10-28T14:43:51.353239Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#2. h p100","metadata":{},"execution_count":null,"outputs":[]}]}