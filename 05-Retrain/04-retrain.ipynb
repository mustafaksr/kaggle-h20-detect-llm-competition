{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style=\"border: 2px solid #555; color:black; border-radius: 10px; background-color: #0074D9; padding: 10px; font-size: 20px; text-align: center;\">Introduction</div>","metadata":{}},{"cell_type":"markdown","source":"**Table Of Content:**\n* [Introduction](#1)\n* [Refactor and Define utils](#2)\n* [Refactor Train](#3)\n* [Sweeps](#4)\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# <div style=\"border: 2px solid #555; color:black; border-radius: 10px; background-color: #0074D9; padding: 10px; font-size: 20px; text-align: center;\">Refactor and Define utils</div>\n* [return top](#1)","metadata":{}},{"cell_type":"code","source":"%%writefile utils.py\nimport json\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport torch\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer, EarlyStoppingCallback\nimport torch.nn.functional as F\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport torch\nfrom datasets import Dataset\nimport io\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport torch.nn.functional as F\nimport wandb\n\ndef read_data():\n    \"\"\"\n    return train , test\n    \"\"\"\n    with open(\"/kaggle/working/artifacts/detect_llm_raw_data:v1/train_df.table.json\") as json_data:\n        data = json.load(json_data)\n        train = pd.DataFrame(data = data[\"data\"],columns=data[\"columns\"])\n        json_data.close()\n\n    with open(\"/kaggle/working/artifacts/detect_llm_raw_data:v1/test_df.table.json\") as json_data:\n        data = json.load(json_data)\n        test = pd.DataFrame(data = data[\"data\"],columns=data[\"columns\"])\n        json_data.close()\n    return train , test\n\ndef preprocess(train=None,test=None):\n    \"\"\"\n    return dataset_train, dataset_test\n    \"\"\"\n    train.fillna(\" \",inplace=True)\n    test.fillna(\" \",inplace=True)\n    train[\"text\"] = train[\"Question\"] + \" \" + train[\"Response\"]\n    test[\"text\"] = test[\"Question\"] + \" \" + test[\"Response\"]\n    df_train = train[[\"target\",\"text\"]]\n    df_test = test[[\"text\"]]\n    dataset_train = Dataset.from_pandas(df_train)\n    dataset_test = Dataset.from_pandas(df_test)\n    \n    return dataset_train, dataset_test\n\n\ndef dataset_tokenize_n_split(train, dataset_train, dataset_test,model_name):\n    \"\"\"\n    return split_train_dataset,split_eval_dataset , tokenized_test , tokenizer\n    \"\"\"\n    tokenizer       = AutoTokenizer.from_pretrained(model_name )\n    def tokenize_function(examples):\n    \n        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n    tokenized_train = dataset_train.map(tokenize_function, batched=True)\n    tokenized_test  = dataset_test.map(tokenize_function, batched=True)\n    tokenized_train = tokenized_train.remove_columns(['text'])\n    tokenized_train = tokenized_train.rename_column(\"target\", \"labels\")\n    tokenized_test = tokenized_test.remove_columns(['text'])\n\n    kf= StratifiedKFold(n_splits=10,shuffle=True,random_state=42)\n    for i , (tr_idx,val_idx) in enumerate(kf.split(train,train.target)):\n        print(f\"Fold : {i}\")\n        print(f\"shape train : {tr_idx.shape}\")\n        print(f\"shape val : {val_idx.shape}\")\n        break\n        \n    \n    split_train_dataset = tokenized_train.select(tr_idx)\n    split_eval_dataset = tokenized_train.select(val_idx)\n\n    return split_train_dataset,split_eval_dataset , tokenized_test , tokenizer, train.iloc[val_idx]\n\ndef predict_fn(model,dataset_ = None):\n    \n    \"\"\"\n    return mean of all_probabilities (m,7)\n    \"\"\"\n    input_ids = dataset_['input_ids']\n    # token_type_ids = dataset_['token_type_ids']\n    attention_mask = dataset_['attention_mask']\n\n    # Move the input tensors to the GPU\n    input_ids = torch.tensor(input_ids).to('cuda:0')\n    # token_type_ids = torch.tensor(token_type_ids).to('cuda:0')\n    attention_mask = torch.tensor(attention_mask).to('cuda:0')\n\n    # Define batch size\n    batch_size = 8\n\n    # Calculate the number of batches\n    num_samples = len(input_ids)\n    num_batches = (num_samples + batch_size - 1) // batch_size\n\n    # Initialize a list to store the softmax probabilities\n    all_probabilities = []\n\n    # Make predictions in batches\n    with torch.no_grad():\n        for batch in range(num_batches):\n            start_idx = batch * batch_size\n            end_idx = min((batch + 1) * batch_size, num_samples)\n\n            batch_input_ids = input_ids[start_idx:end_idx]\n    #         batch_token_type_ids = token_type_ids[start_idx:end_idx]\n            batch_attention_mask = attention_mask[start_idx:end_idx]\n\n            outputs = model(input_ids=batch_input_ids, \n    #                         token_type_ids=batch_token_type_ids, \n                            attention_mask=batch_attention_mask)\n            logits = outputs.logits\n\n            # Apply softmax to get probabilities\n            probabilities = F.softmax(logits, dim=1)\n\n\n            all_probabilities.extend(probabilities.tolist())\n    return np.concatenate(all_probabilities,axis=0).reshape(dataset_.shape[0],7)\n\n\ndef conf_mat(df_val = None,preds_val = None):\n    \"\"\"\n    no return\n    \"\"\"\n    plt.figure(figsize=(8,8))\n    ConfusionMatrixDisplay.from_predictions(df_val.target,np.argmax(preds_val,axis=1))\n    plt.savefig(f\"val_conf_matrix.png\", format=\"png\")\n    plt.show();\n    conf = wandb.Image(data_or_path=\"val_conf_matrix.png\")\n    wandb.log({\"val_conf_matrix\": conf})\ndef create_model(model_name = \"distilroberta-base\",num_labels = 7):\n    \"\"\"\n    return\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n    # Specify the GPU device\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    # Move your model to the GPU\n    model.to(device);\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# <div style=\"border: 2px solid #555; color:black; border-radius: 10px; background-color: #0074D9; padding: 10px; font-size: 20px; text-align: center;\">Refactor Train</div>\n* [return top](#1)","metadata":{}},{"cell_type":"code","source":"%%writefile sweeps_result.py\nepochs = 16\nseed = 42\nbatch_size = 8\nlearning_rate = 0.0001405\nweight_decay = 0.2\nlearning_sch = 'linear'\narchitecture = \"distilroberta-base\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sweeps_result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile requirements.txt\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%writefile train.py\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport torch\nfrom datasets import Dataset\nimport json\nfrom IPython.display import display\nimport wandb\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nfrom transformers import AutoModelForSequenceClassification,TrainerCallback\nfrom transformers import TrainingArguments, Trainer, EarlyStoppingCallback\nimport torch.nn.functional as F\nfrom utils import *\nimport io\nimport matplotlib.pyplot as plt\n\nclass WandbMetricsLogger(TrainerCallback):\n    def on_evaluate(self, args, state, control, model, metrics):\n        # Log metrics to Wandb\n        wandb.log(metrics)\n        \ndefault_config = {\n        'method': 'random',\n        'metric': {\n        'goal': 'minimize', \n        'name': 'eval_loss'\n        },\n    }\n\n\n    # hyperparameters\nparameters_dict = {\n        'epochs': {\n            'value': 2\n            },\n        'seed': {\n            'value': 42\n            },\n        'batch_size': {\n            'values': [4, 8, 16]\n            },\n        'learning_rate': {\n            'distribution': 'log_uniform_values',\n            'min': 1e-4,\n            'max': 2e-3\n        },\n        'weight_decay': {\n            'values': [0.0, 0.2]\n        },\n        'learning_sch': {\n            'values': ['linear','polynomial','cosine']\n        },\n        'architecture': {\n            'values': [\"distilroberta-base\",\"bert-base-uncased\",\"distilbert-base-uncased\"]\n        },\n    }\n\n\ndefault_config['parameters'] = parameters_dict\n\ndef compute_metrics_fn(eval_preds):\n    metrics = dict()\n\n    # Extract the validation loss from eval_preds\n    validation_loss = eval_preds.loss\n    metrics['validation_loss'] = validation_loss\n\n    return metrics\n\ndef parse_args():\n    \"Overriding default argments\"\n    argparser = argparse.ArgumentParser(description='Process hyper-parameters')\n    argparser.add_argument('--batch_size', type=int, default=default_config.get(\"parameters\").get(\"batch_size\").get(\"values\")[-1],\n                           help='batch size')\n    argparser.add_argument('--epochs', type=int, default=default_config.get(\"parameters\").get(\"epochs\").get(\"value\"),\n                           help='number of training epochs')\n    argparser.add_argument('--lr', type=float, default=default_config.get(\"parameters\").get(\"learning_rate\").get(\"min\"),\n                           help='learning rate')\n    argparser.add_argument('--seed', type=int, default=default_config.get(\"parameters\").get(\"seed\").get(\"value\"),\n                           help='random seed')\n    argparser.add_argument('--weight_decay', type=float, default=default_config.get(\"parameters\").get(\"weight_decay\").get(\"values\")[-1],\n                           help='random seed')\n    \n    args = argparser.parse_args()\n    vars(default_config).update(vars(args))\n    return\n\n\n\ndef train(config=None):\n    \n    torch.manual_seed(default_config.get(\"parameters\").get(\"seed\").get(\"value\"))\n    \n    run = wandb.init(\n                project=\"h2o-ai-predict-the-llm-kaggle-competition\", \n                entity=None, \n                   job_type=\"train\",\n                name = \"04-Retrain\",\n                tags = [\"RETRAIN\"]\n                \n    )\n    if \"artifacts\" not in os.listdir():\n        raw_data_at = run.use_artifact('mustafakeser/h2o-ai-predict-the-llm-kaggle-competition/detect_llm_raw_data:v1', \n                                                       type='raw_data')\n        artifact_di = raw_data_at.download()\n    else: pass\n    train , test = read_data()\n    dataset_train, dataset_test = preprocess(train=train,test=test)\n    if config is None:\n        config = wandb.config\n    else:\n        pass \n    split_train_dataset,split_eval_dataset , tokenized_test , tokenizer, df_val = dataset_tokenize_n_split(train,dataset_train, dataset_test,config.architecture)\n\n    \n    \n    \n    model = create_model(model_name =config.architecture ,num_labels = 7)\n    \n    \n    training_args = TrainingArguments(                                \n\n                                output_dir='distilroberta-retrain',\n                                report_to='wandb',  # Turn on Weights & Biases logging\n                                num_train_epochs=config.epochs,\n                                learning_rate=config.learning_rate,\n                                lr_scheduler_type = config.learning_sch,\n                                metric_for_best_model=\"eval_loss\", \n                                load_best_model_at_end=True,\n                                remove_unused_columns=True,\n                                greater_is_better=False,\n                                weight_decay = config.weight_decay,\n                                evaluation_strategy=\"steps\",\n                                logging_steps=100,\n                                per_device_train_batch_size = config.batch_size,\n                                per_device_eval_batch_size = config.batch_size ,\n                                \n                                )\n    early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n    trainer = Trainer(\n                        model=model,\n                        args=training_args,\n                        train_dataset=split_train_dataset,\n                        eval_dataset=split_eval_dataset,\n                        callbacks = [early_stopping],\n                        tokenizer=tokenizer,\n        )\n    trainer.train()\n    \n    val_pred = predict_fn(model, split_eval_dataset)\n    conf_mat(df_val,val_pred)\n    \n# if __name__==\"__main__\":\n# #     wandb.agent(sweep_id, train, count=20)\n#     parse_args()\n#     train(default_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# <div style=\"border: 2px solid #555; color:black; border-radius: 10px; background-color: #0074D9; padding: 10px; font-size: 20px; text-align: center;\">Sweeps</div>\n* [return top](#1)","metadata":{}},{"cell_type":"code","source":"wandb.login(relogin=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(sweeps_result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}